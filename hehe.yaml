---
# Source: temporal/charts/elasticsearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: "elasticsearch-master-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "elasticsearch-master"
---
# Source: temporal/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-5.0.10
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: "6.7.1"
    app.kubernetes.io/managed-by: Helm
  name: temporaltest-grafana
  namespace: default
---
# Source: temporal/charts/prometheus/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.7.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
  name: temporaltest-kube-state-metrics
  namespace: default
imagePullSecrets:
  []
---
# Source: temporal/charts/prometheus/templates/alertmanager-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-alertmanager
  annotations:
    {}
---
# Source: temporal/charts/prometheus/templates/pushgateway-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "pushgateway"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-pushgateway
  annotations:
    {}
---
# Source: temporal/charts/prometheus/templates/server-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "server"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-server
  annotations:
    {}
---
# Source: temporal/charts/cassandra/templates/cassandra-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: temporaltest-cassandra
  namespace: default
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-7.1.2
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  cassandra-password: "YWVJZ2xPbTZHTg=="
---
# Source: temporal/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: temporaltest-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.0.10
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: "6.7.1"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  admin-user: "YWRtaW4="
  admin-password: "dUJRNmlmS2tTUHZNS0p5bUZOY3g1NDZqbThnMTZoOXNxYjZKbzFGcw=="
  ldap-toml: ""
---
# Source: temporal/templates/server-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: temporaltest-default-store
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/part-of: temporal
type: Opaque
data:
  password: "cGFzc3dvcmQ="
---
# Source: temporal/templates/server-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: temporaltest-visibility-store
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/part-of: temporal
type: Opaque
data:
  password: "cGFzc3dvcmQ="
---
# Source: temporal/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: temporaltest-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.0.10
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: "6.7.1"
    app.kubernetes.io/managed-by: Helm
data:
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/data
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning

  datasources.yaml: |
    apiVersion: 1
    datasources:
    - access: proxy
      isDefault: true
      name: TemporalMetrics
      type: prometheus
      url: http://temporaltest-prometheus-server
  dashboardproviders.yaml: |
    apiVersion: 1
    providers:
    - disableDeletion: false
      editable: true
      folder: ""
      name: default
      options:
        path: /var/lib/grafana/dashboards/default
      orgId: 1
      type: file
  download_dashboards.sh: |
    #!/usr/bin/env sh
    set -euf
    mkdir -p /var/lib/grafana/dashboards/default
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      https://raw.githubusercontent.com/temporalio/temporal-dashboards/master/dashboards/10000.json| sed 's|\"datasource\":[^,]*|\"datasource\": \"TemporalMetrics\"|g'\
    > /var/lib/grafana/dashboards/default/clusteroverview-github.json
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      https://raw.githubusercontent.com/temporalio/temporal-dashboards/master/dashboards/common.json| sed 's|\"datasource\":[^,]*|\"datasource\": \"TemporalMetrics\"|g'\
    > /var/lib/grafana/dashboards/default/common-github.json
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      https://raw.githubusercontent.com/temporalio/temporal-dashboards/master/dashboards/frontend.json| sed 's|\"datasource\":[^,]*|\"datasource\": \"TemporalMetrics\"|g'\
    > /var/lib/grafana/dashboards/default/frontend-github.json
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      https://raw.githubusercontent.com/temporalio/temporal-dashboards/master/dashboards/history.json| sed 's|\"datasource\":[^,]*|\"datasource\": \"TemporalMetrics\"|g'\
    > /var/lib/grafana/dashboards/default/history-github.json
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      https://raw.githubusercontent.com/temporalio/temporal-dashboards/master/dashboards/matching.json| sed 's|\"datasource\":[^,]*|\"datasource\": \"TemporalMetrics\"|g'\
    > /var/lib/grafana/dashboards/default/matching-github.json
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      https://raw.githubusercontent.com/temporalio/temporal-dashboards/master/dashboards/temporal.json| sed 's|\"datasource\":[^,]*|\"datasource\": \"TemporalMetrics\"|g'\
    > /var/lib/grafana/dashboards/default/temporal-github.json
---
# Source: temporal/charts/grafana/templates/dashboards-json-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: temporaltest-grafana-dashboards-default
  namespace: default
  labels:
    helm.sh/chart: grafana-5.0.10
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: "6.7.1"
    app.kubernetes.io/managed-by: Helm
    dashboard-provider: default
data:
  {}
---
# Source: temporal/charts/prometheus/templates/alertmanager-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-alertmanager
data:
  alertmanager.yml: |
    global: {}
    receivers:
    - name: default-receiver
    route:
      group_interval: 5m
      group_wait: 10s
      receiver: default-receiver
      repeat_interval: 3h
---
# Source: temporal/charts/prometheus/templates/server-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "server"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-server
data:
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 1m
      scrape_timeout: 10s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets:
        - localhost:9090
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-apiservers
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes-cadvisor
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: kubernetes_node
    - job_name: kubernetes-service-endpoints-slow
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: kubernetes_node
      scrape_interval: 5m
      scrape_timeout: 30s
    - honor_labels: true
      job_name: prometheus-pushgateway
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        regex: pushgateway
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
    - job_name: kubernetes-services
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module:
        - http_2xx
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
      - source_labels:
        - __address__
        target_label: __param_target
      - replacement: blackbox
        target_label: __address__
      - source_labels:
        - __param_target
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
    - job_name: kubernetes-pods-slow
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
      scrape_interval: 5m
      scrape_timeout: 30s
    alerting:
      alertmanagers:
      - kubernetes_sd_configs:
          - role: pod
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          regex: default
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_app]
          regex: prometheus
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_component]
          regex: alertmanager
          action: keep
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_probe]
          regex: .*
          action: keep
        - source_labels: [__meta_kubernetes_pod_container_port_number]
          regex:
          action: drop
  recording_rules.yml: |
    {}
  rules: |
    {}
---
# Source: temporal/templates/server-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "temporaltest-config"
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/part-of: temporal
data:
  config_template.yaml: |-
    log:
      stdout: true
      level: "debug,info"

    persistence:
      defaultStore: default
      visibilityStore: visibility
      advancedVisibilityStore: es-visibility
      numHistoryShards: 512
      datastores:
        default:
          cassandra:
            hosts: temporaltest-cassandra.default.svc.cluster.local,temporaltest-cassandra.default.svc.cluster.local,temporaltest-cassandra.default.svc.cluster.local,
            port: 9042
            password: {{ .Env.TEMPORAL_STORE_PASSWORD }}
            consistency:
              default:
                consistency: local_quorum
                serialConsistency: local_serial
            keyspace: temporal
            replicationFactor: 1
            user: user
        visibility:
          cassandra:
            hosts: temporaltest-cassandra.default.svc.cluster.local,temporaltest-cassandra.default.svc.cluster.local,temporaltest-cassandra.default.svc.cluster.local,
            port: 9042
            password: {{ .Env.TEMPORAL_VISIBILITY_STORE_PASSWORD }}
            consistency:
              default:
                consistency: local_quorum
                serialConsistency: local_serial
            keyspace: temporal_visibility
            replicationFactor: 1
            user: user
        es-visibility:
            elasticsearch:
                url:
                    scheme: "http"
                    host: "elasticsearch-master-headless:9200"
                username: ""
                password: ""
                indices:
                    visibility: "temporal-visibility-dev"

    global:
      membership:
        name: temporal
        maxJoinDuration: 30s
        broadcastAddress: {{ default .Env.POD_IP "0.0.0.0" }}

      pprof:
        port: 7936


    services:
      frontend:
        rpc:
          grpcPort: 7233
          membershipPort: 6933
          bindOnIP: "0.0.0.0"
        metrics:
          tags:
            type: frontend
          prometheus:
            timerType: histogram
            listenAddress: "0.0.0.0:9090"

      history:
        rpc:
          grpcPort: 7934
          membershipPort: 6934
          bindOnIP: "0.0.0.0"
        metrics:
          tags:
            type: history
          prometheus:
            timerType: histogram
            listenAddress: "0.0.0.0:9090"

      matching:
        rpc:
          grpcPort: 7935
          membershipPort: 6935
          bindOnIP: "0.0.0.0"
        metrics:
          tags:
            type: matching
          prometheus:
            timerType: histogram
            listenAddress: "0.0.0.0:9090"

      worker:
        rpc:
          grpcPort: 7239
          membershipPort: 6939
          bindOnIP: "0.0.0.0"
        metrics:
          tags:
            type: worker
          prometheus:
            timerType: histogram
            listenAddress: "0.0.0.0:9090"
    kafka:
        tls:
            enabled: false
        clusters:
            test:
                brokers:
                    - temporaltest-kafka-headless:9092
        topics:
            temporal-visibility-dev:
                cluster: test
            temporal-visibility-dev-dlq:
                cluster: test
        applications:
            visibility:
                topic: temporal-visibility-dev
                dlq-topic: temporal-visibility-dev-dlq
    clusterMetadata:
      enableGlobalDomain: false
      failoverVersionIncrement: 10
      masterClusterName: "active"
      currentClusterName: "active"
      clusterInformation:
        active:
          enabled: true
          initialFailoverVersion: 1
          rpcName: "temporal-frontend"
          rpcAddress: "127.0.0.1:7933"

    dcRedirectionPolicy:
      policy: "noop"
      toDC: ""

    archival:
      status: "disabled"

    publicClient:
      hostPort: "temporaltest-frontend:7233"

    dynamicConfigClient:
      filepath: "/etc/temporal/dynamic_config/dynamic_config.yaml"
      pollInterval: "10s"
---
# Source: temporal/templates/server-dynamicconfigmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "temporaltest-dynamic-config"
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/part-of: temporal
data:
  dynamic_config.yaml: |-
---
# Source: temporal/charts/prometheus/templates/alertmanager-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-alertmanager
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "2Gi"
---
# Source: temporal/charts/prometheus/templates/server-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    component: "server"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-server
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "8Gi"
---
# Source: temporal/charts/prometheus/charts/kube-state-metrics/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.7.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
  name: temporaltest-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]
---
# Source: temporal/charts/prometheus/templates/alertmanager-clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-alertmanager
rules:
  []
---
# Source: temporal/charts/prometheus/templates/pushgateway-clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    component: "pushgateway"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-pushgateway
rules:
  []
---
# Source: temporal/charts/prometheus/templates/server-clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    component: "server"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-server
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
      - ingresses
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
      - ingresses
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
# Source: temporal/charts/prometheus/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.7.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
  name: temporaltest-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: temporaltest-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: temporaltest-kube-state-metrics
  namespace: default
---
# Source: temporal/charts/prometheus/templates/alertmanager-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-alertmanager
subjects:
  - kind: ServiceAccount
    name: temporaltest-prometheus-alertmanager
    namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: temporaltest-prometheus-alertmanager
---
# Source: temporal/charts/prometheus/templates/pushgateway-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "pushgateway"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-pushgateway
subjects:
  - kind: ServiceAccount
    name: temporaltest-prometheus-pushgateway
    namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: temporaltest-prometheus-pushgateway
---
# Source: temporal/charts/prometheus/templates/server-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "server"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-server
subjects:
  - kind: ServiceAccount
    name: temporaltest-prometheus-server
    namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: temporaltest-prometheus-server
---
# Source: temporal/charts/cassandra/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-cassandra-headless
  namespace: default
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-7.1.2
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: intra
      port: 7000
      targetPort: intra
    - name: tls
      port: 7001
      targetPort: tls
    - name: jmx
      port: 7199
      targetPort: jmx
    - name: cql
      port: 9042
      targetPort: cql
    - name: thrift
      port: 9160
      targetPort: thrift
  selector:
    app.kubernetes.io/name: cassandra
    app.kubernetes.io/instance: temporaltest
---
# Source: temporal/charts/cassandra/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-cassandra
  namespace: default
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-7.1.2
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: cql
      port: 9042
      targetPort: cql
      nodePort: null
    - name: thrift
      port: 9160
      targetPort: thrift
      nodePort: null
    - name: metrics
      port: 8080
      nodePort: null
  selector:
    app.kubernetes.io/name: cassandra
    app.kubernetes.io/instance: temporaltest
---
# Source: temporal/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "temporaltest"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    heritage: "Helm"
    release: "temporaltest"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: temporal/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master-headless
  labels:
    heritage: "Helm"
    release: "temporaltest"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "elasticsearch-master"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: temporal/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.0.10
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: "6.7.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000

  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: temporaltest
---
# Source: temporal/charts/kafka/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-zookeeper-headless
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-5.4.3
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: client
      port: 2181
      targetPort: client
    - name: follower
      port: 2888
      targetPort: follower
    - name: election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: zookeeper
---
# Source: temporal/charts/kafka/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-zookeeper
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-5.4.3
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  ports:
    - name: client
      port: 2181
      targetPort: client
    - name: follower
      port: 2888
      targetPort: follower
    - name: election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: zookeeper
---
# Source: temporal/charts/kafka/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-kafka-headless
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-7.2.9
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: kafka
      port: 9092
      targetPort: kafka
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: kafka
---
# Source: temporal/charts/kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-kafka
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-7.2.9
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
  annotations: 
    {}
spec:
  type: ClusterIP
  ports:
    - name: kafka
      port: 9092
      targetPort: kafka
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: kafka
---
# Source: temporal/charts/prometheus/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-kube-state-metrics
  namespace: default
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: "kube-state-metrics-2.7.2"
    app.kubernetes.io/instance: "temporaltest"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  selector:
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: temporaltest
---
# Source: temporal/charts/prometheus/templates/alertmanager-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-alertmanager
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9093
  selector:
    component: "alertmanager"
    app: prometheus
    release: temporaltest
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: temporal/charts/prometheus/templates/pushgateway-service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/probe: pushgateway
  labels:
    component: "pushgateway"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-pushgateway
spec:
  ports:
    - name: http
      port: 9091
      protocol: TCP
      targetPort: 9091
  selector:
    component: "pushgateway"
    app: prometheus
    release: temporaltest
  type: "ClusterIP"
---
# Source: temporal/charts/prometheus/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: "server"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-server
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
  selector:
    component: "server"
    app: prometheus
    release: temporaltest
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: temporal/templates/admintools-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-admintools
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: admintools
    app.kubernetes.io/part-of: temporal
spec:
  type: ClusterIP 
  ports:
    - port: 22
      targetPort: 22
      protocol: TCP
      name: ssh

  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: admintools
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-frontend
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: frontend
    app.kubernetes.io/part-of: temporal
spec:
  type: ClusterIP
  ports:
    - port: 7233
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: frontend
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-frontend-headless
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: frontend
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-frontend
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 7233
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
    - port: 9090
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: frontend
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-matching-headless
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: matching
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-matching
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 7935
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
    - port: 9090
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: matching
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-history-headless
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: history
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-history
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 7934
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
    - port: 9090
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: history
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-worker-headless
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: worker
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-worker
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 7239
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
    - port: 9090
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: worker
---
# Source: temporal/templates/web-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: temporaltest-web
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: web
    app.kubernetes.io/part-of: temporal
spec:
  type: ClusterIP
  ports:
    - port: 8088
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/component: web
---
# Source: temporal/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: temporaltest-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.0.10
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: "6.7.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: temporaltest
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: temporaltest
      annotations:
        checksum/config: 3609580b833b883e56a866e799435897ee6cef3fe25e6510a39ba3e88c6bc68c
        checksum/dashboards-json-config: 3389f27e5675b60c26d0c729e599c9016a4fc96730dc863b648fccdef86bf4bb
        checksum/sc-dashboard-provider-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/secret: 77a579b8685b63dc39bc55ce654ba5e954b3a5c71fb6a74f05287a4978adf2b8
    spec:
      
      serviceAccountName: temporaltest-grafana
      securityContext:
        fsGroup: 472
        runAsUser: 472
      initContainers:
        - name: download-dashboards
          image: "curlimages/curl:7.68.0"
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh"]
          args: [ "-c", "mkdir -p /var/lib/grafana/dashboards/default && /bin/sh /etc/grafana/download_dashboards.sh" ]
          resources:
            {}
          env:
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/download_dashboards.sh"
              subPath: download_dashboards.sh
            - name: storage
              mountPath: "/var/lib/grafana"
      containers:
        - name: grafana
          image: "grafana/grafana:6.7.1"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: config
              mountPath: "/etc/grafana/provisioning/datasources/datasources.yaml"
              subPath: datasources.yaml
            - name: config
              mountPath: "/etc/grafana/provisioning/dashboards/dashboardproviders.yaml"
              subPath: dashboardproviders.yaml
          ports:
            - name: service
              containerPort: 80
              protocol: TCP
            - name: grafana
              containerPort: 3000
              protocol: TCP
          env:
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: temporaltest-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: temporaltest-grafana
                  key: admin-password
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: temporaltest-grafana
        - name: dashboards-default
          configMap:
            name: temporaltest-grafana-dashboards-default
        - name: storage
          emptyDir: {}
---
# Source: temporal/charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: temporaltest-kube-state-metrics
  namespace: default
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: "kube-state-metrics-2.7.2"
    app.kubernetes.io/instance: "temporaltest"
    app.kubernetes.io/managed-by: "Helm"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kube-state-metrics
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: "temporaltest"
    spec:
      hostNetwork: false
      serviceAccountName: temporaltest-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsUser: 65534
      containers:
      - name: kube-state-metrics
        args:

        - --collectors=certificatesigningrequests


        - --collectors=configmaps


        - --collectors=cronjobs


        - --collectors=daemonsets


        - --collectors=deployments


        - --collectors=endpoints


        - --collectors=horizontalpodautoscalers


        - --collectors=ingresses


        - --collectors=jobs


        - --collectors=limitranges



        - --collectors=namespaces



        - --collectors=nodes


        - --collectors=persistentvolumeclaims


        - --collectors=persistentvolumes


        - --collectors=poddisruptionbudgets


        - --collectors=pods


        - --collectors=replicasets


        - --collectors=replicationcontrollers


        - --collectors=resourcequotas


        - --collectors=secrets


        - --collectors=services


        - --collectors=statefulsets


        - --collectors=storageclasses






        imagePullPolicy: IfNotPresent
        image: "quay.io/coreos/kube-state-metrics:v1.9.5"
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
---
# Source: temporal/charts/prometheus/templates/alertmanager-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-alertmanager
spec:
  selector:
    matchLabels:
      component: "alertmanager"
      app: prometheus
      release: temporaltest
  replicas: 1
  template:
    metadata:
      labels:
        component: "alertmanager"
        app: prometheus
        release: temporaltest
        chart: prometheus-11.0.4
        heritage: Helm
    spec:
      serviceAccountName: temporaltest-prometheus-alertmanager
      containers:
        - name: prometheus-alertmanager
          image: "prom/alertmanager:v0.20.0"
          imagePullPolicy: "IfNotPresent"
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
          args:
            - --config.file=/etc/config/alertmanager.yml
            - --storage.path=/data
            - --cluster.advertise-address=$(POD_IP):6783
            - --web.external-url=http://localhost:9093

          ports:
            - containerPort: 9093
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9093
            initialDelaySeconds: 30
            timeoutSeconds: 30
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: "/data"
              subPath: ""
        - name: prometheus-alertmanager-configmap-reload
          image: "jimmidyson/configmap-reload:v0.3.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://127.0.0.1:9093/-/reload
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      volumes:
        - name: config-volume
          configMap:
            name: temporaltest-prometheus-alertmanager
        - name: storage-volume
          persistentVolumeClaim:
            claimName: temporaltest-prometheus-alertmanager
---
# Source: temporal/charts/prometheus/templates/pushgateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: "pushgateway"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-pushgateway
spec:
  selector:
    matchLabels:
      component: "pushgateway"
      app: prometheus
      release: temporaltest
  replicas: 1
  template:
    metadata:
      labels:
        component: "pushgateway"
        app: prometheus
        release: temporaltest
        chart: prometheus-11.0.4
        heritage: Helm
    spec:
      serviceAccountName: temporaltest-prometheus-pushgateway
      containers:
        - name: prometheus-pushgateway
          image: "prom/pushgateway:v1.0.1"
          imagePullPolicy: "IfNotPresent"
          args:
          ports:
            - containerPort: 9091
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          resources:
            {}
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
---
# Source: temporal/charts/prometheus/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: "server"
    app: prometheus
    release: temporaltest
    chart: prometheus-11.0.4
    heritage: Helm
  name: temporaltest-prometheus-server
spec:
  selector:
    matchLabels:
      component: "server"
      app: prometheus
      release: temporaltest
  replicas: 1
  template:
    metadata:
      labels:
        component: "server"
        app: prometheus
        release: temporaltest
        chart: prometheus-11.0.4
        heritage: Helm
    spec:
      serviceAccountName: temporaltest-prometheus-server
      containers:
        - name: prometheus-server-configmap-reload
          image: "jimmidyson/configmap-reload:v0.3.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://127.0.0.1:9090/-/reload
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true

        - name: prometheus-server
          image: "prom/prometheus:v2.16.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --storage.tsdb.retention.time=15d
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
            initialDelaySeconds: 30
            timeoutSeconds: 30
            failureThreshold: 3
            successThreshold: 1
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
            initialDelaySeconds: 30
            timeoutSeconds: 30
            failureThreshold: 3
            successThreshold: 1
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: temporaltest-prometheus-server
        - name: storage-volume
          persistentVolumeClaim:
            claimName: temporaltest-prometheus-server
---
# Source: temporal/templates/admintools-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: temporaltest-admintools
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: admintools
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: temporaltest
      app.kubernetes.io/component: admintools
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.2.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/version: 1.5.0
        app.kubernetes.io/component: admintools
        app.kubernetes.io/part-of: temporal
    spec:
      containers:
        - name: admin-tools
          image: "temporaliotest/admin-tools:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 22
              protocol: TCP
          env:
            - name: TEMPORAL_CLI_ADDRESS
              value: temporaltest-frontend:7233
          livenessProbe:
              exec:
                command:
                - ls
                - /
              initialDelaySeconds: 5
              periodSeconds: 5
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: temporaltest-frontend
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: frontend
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: temporaltest
      app.kubernetes.io/component: frontend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.2.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/version: 1.5.0
        app.kubernetes.io/component: frontend
        app.kubernetes.io/part-of: temporal
      annotations:
        checksum/config: 3da1c8bf9cd7c2a85b8285bcad31fbb434fb09aea61da863d2f251ad76251d31
        prometheus.io/job: temporal-frontend
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
    spec:
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup temporaltest-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
        - name: check-cassandra-temporal-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal$; do echo waiting for default keyspace to become ready; sleep 1; done;']
        - name: check-cassandra-visibility-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal_visibility$; do echo waiting for visibility keyspace to become ready; sleep 1; done;']
      containers:
        - name: temporal-frontend
          image: "temporaliotest/server:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ENABLE_ES
              value: "true"
            - name: ES_SEEDS
              value: "elasticsearch-master-headless"
            - name: ES_PORT
              value: "9200"
            - name: ES_SCHEME
              value: "http"
            - name: ES_VIS_INDEX
              value: "temporal-visibility-dev"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              value: ""
            - name: SERVICES
              value: frontend
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: temporaltest-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: temporaltest-visibility-store
                  key: password
          ports:
            - name: rpc
              containerPort: 7233
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          livenessProbe:
             initialDelaySeconds: 150
             tcpSocket:
               port: rpc
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
      volumes:
        - name: config
          configMap:
            name: "temporaltest-config"
        - name: dynamic-config
          configMap:
            name: "temporaltest-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                  - history
                  - matching
                  - worker
              topologyKey: kubernetes.io/hostname
            weight: 50
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - cassandra
              topologyKey: kubernetes.io/hostname
            weight: 100
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - elasticsearch-master
              topologyKey: kubernetes.io/hostname
            weight: 75
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - frontend
            topologyKey: kubernetes.io/hostname
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: temporaltest-history
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: history
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: temporaltest
      app.kubernetes.io/component: history
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.2.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/version: 1.5.0
        app.kubernetes.io/component: history
        app.kubernetes.io/part-of: temporal
      annotations:
        checksum/config: 3da1c8bf9cd7c2a85b8285bcad31fbb434fb09aea61da863d2f251ad76251d31
        prometheus.io/job: temporal-history
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
    spec:
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup temporaltest-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
        - name: check-cassandra-temporal-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal$; do echo waiting for default keyspace to become ready; sleep 1; done;']
        - name: check-cassandra-visibility-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal_visibility$; do echo waiting for visibility keyspace to become ready; sleep 1; done;']
      containers:
        - name: temporal-history
          image: "temporaliotest/server:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ENABLE_ES
              value: "true"
            - name: ES_SEEDS
              value: "elasticsearch-master-headless"
            - name: ES_PORT
              value: "9200"
            - name: ES_SCHEME
              value: "http"
            - name: ES_VIS_INDEX
              value: "temporal-visibility-dev"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              value: ""
            - name: SERVICES
              value: history
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: temporaltest-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: temporaltest-visibility-store
                  key: password
          ports:
            - name: rpc
              containerPort: 7934
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          livenessProbe:
             initialDelaySeconds: 150
             tcpSocket:
               port: rpc
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
      volumes:
        - name: config
          configMap:
            name: "temporaltest-config"
        - name: dynamic-config
          configMap:
            name: "temporaltest-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                  - frontend
                  - matching
                  - worker
              topologyKey: kubernetes.io/hostname
            weight: 50
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - cassandra
              topologyKey: kubernetes.io/hostname
            weight: 100
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - elasticsearch-master
              topologyKey: kubernetes.io/hostname
            weight: 75
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - history
            topologyKey: kubernetes.io/hostname
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: temporaltest-matching
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: matching
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: temporaltest
      app.kubernetes.io/component: matching
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.2.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/version: 1.5.0
        app.kubernetes.io/component: matching
        app.kubernetes.io/part-of: temporal
      annotations:
        checksum/config: 3da1c8bf9cd7c2a85b8285bcad31fbb434fb09aea61da863d2f251ad76251d31
        prometheus.io/job: temporal-matching
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
    spec:
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup temporaltest-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
        - name: check-cassandra-temporal-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal$; do echo waiting for default keyspace to become ready; sleep 1; done;']
        - name: check-cassandra-visibility-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal_visibility$; do echo waiting for visibility keyspace to become ready; sleep 1; done;']
      containers:
        - name: temporal-matching
          image: "temporaliotest/server:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ENABLE_ES
              value: "true"
            - name: ES_SEEDS
              value: "elasticsearch-master-headless"
            - name: ES_PORT
              value: "9200"
            - name: ES_SCHEME
              value: "http"
            - name: ES_VIS_INDEX
              value: "temporal-visibility-dev"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              value: ""
            - name: SERVICES
              value: matching
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: temporaltest-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: temporaltest-visibility-store
                  key: password
          ports:
            - name: rpc
              containerPort: 7935
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          livenessProbe:
             initialDelaySeconds: 150
             tcpSocket:
               port: rpc
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
      volumes:
        - name: config
          configMap:
            name: "temporaltest-config"
        - name: dynamic-config
          configMap:
            name: "temporaltest-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                  - frontend
                  - history
                  - worker
              topologyKey: kubernetes.io/hostname
            weight: 50
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - cassandra
              topologyKey: kubernetes.io/hostname
            weight: 100
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - elasticsearch-master
              topologyKey: kubernetes.io/hostname
            weight: 75
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - matching
            topologyKey: kubernetes.io/hostname
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: temporaltest-worker
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: worker
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: temporaltest
      app.kubernetes.io/component: worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.2.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/version: 1.5.0
        app.kubernetes.io/component: worker
        app.kubernetes.io/part-of: temporal
      annotations:
        checksum/config: 3da1c8bf9cd7c2a85b8285bcad31fbb434fb09aea61da863d2f251ad76251d31
        prometheus.io/job: temporal-worker
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
    spec:
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup temporaltest-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
        - name: check-cassandra-temporal-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal$; do echo waiting for default keyspace to become ready; sleep 1; done;']
        - name: check-cassandra-visibility-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal_visibility$; do echo waiting for visibility keyspace to become ready; sleep 1; done;']
      containers:
        - name: temporal-worker
          image: "temporaliotest/server:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ENABLE_ES
              value: "true"
            - name: ES_SEEDS
              value: "elasticsearch-master-headless"
            - name: ES_PORT
              value: "9200"
            - name: ES_SCHEME
              value: "http"
            - name: ES_VIS_INDEX
              value: "temporal-visibility-dev"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              value: ""
            - name: SERVICES
              value: worker
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: temporaltest-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: temporaltest-visibility-store
                  key: password
          ports:
            - name: rpc
              containerPort: 7239
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
      volumes:
        - name: config
          configMap:
            name: "temporaltest-config"
        - name: dynamic-config
          configMap:
            name: "temporaltest-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                  - frontend
                  - matching
                  - history
              topologyKey: kubernetes.io/hostname
            weight: 50
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - cassandra
              topologyKey: kubernetes.io/hostname
            weight: 100
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - elasticsearch-master
              topologyKey: kubernetes.io/hostname
            weight: 75
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - worker
            topologyKey: kubernetes.io/hostname
---
# Source: temporal/templates/web-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: temporaltest-web
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: web
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: temporaltest
      app.kubernetes.io/component: web
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.2.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/version: 1.5.0
        app.kubernetes.io/component: web
        app.kubernetes.io/part-of: temporal
    spec:
      containers:
        - name: temporal-web
          image: "temporaliotest/tweb:dbe7057c82a717243cf7a2e02e0b6d0555c9db5e"
          imagePullPolicy: IfNotPresent
          env:
            - name: TEMPORAL_GRPC_ENDPOINT
              value: "temporaltest-frontend.default.svc:7233"

          ports:
            - name: http
              containerPort: 8088
              protocol: TCP
          resources:
            {}
---
# Source: temporal/charts/cassandra/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: temporaltest-cassandra
  namespace: default
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-7.1.2
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: cassandra
      app.kubernetes.io/instance: temporaltest
  serviceName: temporaltest-cassandra-headless
  podManagementPolicy: OrderedReady
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cassandra
        helm.sh/chart: cassandra-7.1.2
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/managed-by: Helm
    spec:
      
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                  - frontend
                  - history
                  - matching
                  - worker
              topologyKey: kubernetes.io/hostname
            weight: 100
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - elasticsearch-master
              topologyKey: kubernetes.io/hostname
            weight: 50
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                  - kafka
                  - zookeeper
              topologyKey: kubernetes.io/hostname
            weight: 15
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - prometheus
              topologyKey: kubernetes.io/hostname
            weight: 5
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - grafana
              topologyKey: kubernetes.io/hostname
            weight: 1
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cassandra
            topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1001
      containers:
        - name: cassandra
          command:
            - bash
            - -ec
            - |
              # Node 0 is the password seeder
              if [[ $HOSTNAME =~ (.*)-0$ ]]; then
                  echo "Setting node as password seeder"
                  export CASSANDRA_PASSWORD_SEEDER=yes
              else
                  # Only node 0 will execute the startup initdb scripts
                  export CASSANDRA_IGNORE_INITDB_SCRIPTS=1
              fi
              /opt/bitnami/scripts/cassandra/entrypoint.sh /opt/bitnami/scripts/cassandra/run.sh
          image: docker.io/bitnami/cassandra:3.11.3
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: CASSANDRA_CLUSTER_NAME
              value: cassandra
            - name: CASSANDRA_SEEDS
              value: "temporaltest-cassandra-0.temporaltest-cassandra-headless.default.svc.cluster.local"
            - name: CASSANDRA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: temporaltest-cassandra
                  key: cassandra-password
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CASSANDRA_USER
              value: "cassandra"
            - name: CASSANDRA_NUM_TOKENS
              value: "256"
            - name: CASSANDRA_DATACENTER
              value: dc1
            - name: CASSANDRA_ENDPOINT_SNITCH
              value: SimpleSnitch
            - name: CASSANDRA_RACK
              value: rack1
            - name: CASSANDRA_ENABLE_RPC
              value: "true"
          envFrom:
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  nodetool status
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  nodetool status | grep -E "^UN\\s+${POD_IP}"
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          lifecycle:
            preStop:
              exec:
                command:
                  - bash
                  - -ec
                  - nodetool decommission
          ports:
            - name: intra
              containerPort: 7000
            - name: tls
              containerPort: 7001
            - name: jmx
              containerPort: 7199
            - name: cql
              containerPort: 9042
            - name: thrift
              containerPort: 9160
          resources: 
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/cassandra
            
      volumes:
        - name: data
          emptyDir: {}
---
# Source: temporal/charts/elasticsearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "temporaltest"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    esMajorVersion: "6"
spec:
  serviceName: elasticsearch-master-headless
  selector:
    matchLabels:
      app: "elasticsearch-master"
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      name: "elasticsearch-master"
      labels:
        heritage: "Helm"
        release: "temporaltest"
        chart: "elasticsearch"
        app: "elasticsearch-master"
      annotations:
        
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "elasticsearch-master"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:6.8.8"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:6.8.8"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: 'wait_for_status=green&timeout=1s' )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                http () {
                    local path="${1}"
                    if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                      BASIC_AUTH="-u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                    else
                      BASIC_AUTH=''
                    fi
                    curl -XGET -s -k --fail ${BASIC_AUTH} http://127.0.0.1:9200${path}
                }

                if [ -f "${START_FILE}" ]; then
                    echo 'Elasticsearch is already running, lets check the node is healthy and there are master nodes available'
                    http "/_cluster/health?timeout=0s"
                else
                    echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                    if http "/_cluster/health?wait_for_status=green&timeout=1s" ; then
                        touch ${START_FILE}
                        exit 0
                    else
                        echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                        exit 1
                    fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 1000m
            memory: 2Gi
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.zen.minimum_master_nodes
            value: "2"
          - name: discovery.zen.ping.unicast.hosts
            value: "elasticsearch-master-headless"
          - name: cluster.name
            value: "elasticsearch"
          - name: network.host
            value: "0.0.0.0"
          - name: ES_JAVA_OPTS
            value: "-Xmx1g -Xms1g"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "true"
        volumeMounts:
---
# Source: temporal/charts/kafka/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: temporaltest-zookeeper
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-5.4.3
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  serviceName: temporaltest-zookeeper-headless
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: temporaltest
      app.kubernetes.io/component: zookeeper
  template:
    metadata:
      name: temporaltest-zookeeper
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-5.4.3
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      
      securityContext:
        fsGroup: 1001
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.5.7-debian-10-r11
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - bash
            - -ec
            - |
                # Execute entrypoint as usual after obtaining ZOO_SERVER_ID based on POD hostname
                HOSTNAME=`hostname -s`
                if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
                  ORD=${BASH_REMATCH[2]}
                  export ZOO_SERVER_ID=$((ORD+1))
                else
                  echo "Failed to get index from hostname $HOST"
                  exit 1
                fi
                exec /entrypoint.sh /run.sh
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_SERVERS
              value: temporaltest-zookeeper-0.temporaltest-zookeeper-headless.default.svc.cluster.local:2888:3888 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            tcpSocket:
              port: client
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            tcpSocket:
              port: client
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: temporal/charts/kafka/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: temporaltest-kafka
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-7.2.9
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
    role: kafka
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: temporaltest
      app.kubernetes.io/component: kafka
  serviceName: temporaltest-kafka-headless
  podManagementPolicy: "Parallel"
  replicas: 1
  updateStrategy:
    type: "RollingUpdate"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-7.2.9
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka
    spec:      
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
      nodeSelector:
        {}
      tolerations:
        []
      affinity:
        {}
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:2.4.0-debian-10-r31
          imagePullPolicy: "IfNotPresent"
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_CFG_ZOOKEEPER_CONNECT
              value: temporaltest-zookeeper
            - name: KAFKA_PORT_NUMBER
              value: "9092"
            - name: KAFKA_CFG_LISTENERS
              value: "PLAINTEXT://:$(KAFKA_PORT_NUMBER)"
            - name: KAFKA_CFG_ADVERTISED_LISTENERS
              value: 'PLAINTEXT://$(MY_POD_NAME).temporaltest-kafka-headless.default.svc.cluster.local:$(KAFKA_PORT_NUMBER)'
            - name: ALLOW_PLAINTEXT_LISTENER
              value: "yes"
            - name: KAFKA_CFG_BROKER_ID
              value: "-1"
            - name: KAFKA_CFG_DELETE_TOPIC_ENABLE
              value: "false"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES
              value: "10000"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MS
              value: "1000"
            - name: KAFKA_CFG_LOG_RETENTION_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_RETENTION_CHECK_INTERVALS_MS
              value: "300000"
            - name: KAFKA_CFG_LOG_RETENTION_HOURS
              value: "168"
            - name: KAFKA_CFG_MESSAGE_MAX_BYTES
              value: "1000012"
            - name: KAFKA_CFG_LOG_SEGMENT_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_DIRS
              value: /bitnami/kafka/data
            - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM
              value: "https"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
              value: "1"
            - name: KAFKA_CFG_NUM_IO_THREADS
              value: "8"
            - name: KAFKA_CFG_NUM_NETWORK_THREADS
              value: "3"
            - name: KAFKA_CFG_NUM_PARTITIONS
              value: "1"
            - name: KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR
              value: "1"
            - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
              value: "104857600"
            - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS
              value: "6000"
          ports:
            - name: kafka
              containerPort: 9092
          livenessProbe:
            tcpSocket:
              port: kafka
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 2
          readinessProbe:
            tcpSocket:
              port: kafka
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
        
      volumes:
        
        
        
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: temporal/charts/elasticsearch/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "temporaltest-thzcf-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
  - name: "temporaltest-asllt-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:6.8.8"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
---
# Source: temporal/templates/server-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: temporaltest-schema-setup
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: database
    app.kubernetes.io/part-of: temporal
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-weight": "0"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
spec:
  backoffLimit: 100
  template:
    metadata:
      name: temporaltest-schema-setup
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.2.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/version: 1.5.0
        app.kubernetes.io/component: database
        app.kubernetes.io/part-of: temporal
    spec:
      restartPolicy: "OnFailure"
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup temporaltest-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
        - name: create-default-store
          image: "temporaliotest/server:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          args: ['sh', '-c', 'temporal-cassandra-tool create -k temporal --replication-factor 1']
          env:
            - name: CASSANDRA_HOST
              value: temporaltest-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
        - name: create-visibility-store
          image: "temporaliotest/server:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          args: ['sh', '-c', 'temporal-cassandra-tool create -k temporal_visibility --replication-factor 1']
          env:
            - name: CASSANDRA_HOST
              value: temporaltest-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal_visibility
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
      containers:
        - name: default-schema
          image: "temporaliotest/server:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          args: ["temporal-cassandra-tool", "setup-schema", "-v", "0.0"]
          env:
            - name: CASSANDRA_HOST
              value: temporaltest-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
        - name: visibility-schema
          image: "temporaliotest/server:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          args: ["temporal-cassandra-tool", "setup-schema", "-v", "0.0"]
          env:
            - name: CASSANDRA_HOST
              value: temporaltest-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal_visibility
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
---
# Source: temporal/templates/server-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: temporaltest-schema-update
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.2.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: temporaltest
    app.kubernetes.io/version: 1.5.0
    app.kubernetes.io/component: database
    app.kubernetes.io/part-of: temporal
  annotations:
    "helm.sh/hook": post-install,pre-upgrade
    "helm.sh/hook-weight": "1"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
spec:
  backoffLimit: 100
  template:
    metadata:
      name: temporaltest-schema-update
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.2.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: temporaltest
        app.kubernetes.io/version: 1.5.0
        app.kubernetes.io/component: database
        app.kubernetes.io/part-of: temporal
    spec:
      restartPolicy: "OnFailure"
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup temporaltest-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh temporaltest-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
      containers:
        - name: default-schema
          image: "temporaliotest/server:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          # args: ["temporal-cassandra-tool", "update-schema", "-d", "/etc/temporal/schema/cassandra/temporal/versioned"]
          args: ['sh', '-c', 'temporal-cassandra-tool update-schema -d /etc/temporal/schema/cassandra/temporal/versioned']
          env:
            - name: CASSANDRA_HOST
              value: temporaltest-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
        - name: visibility-schema
          image: "temporaliotest/server:4e5554208c3fd24a5d92a83895a27b27795e6bd8"
          imagePullPolicy: IfNotPresent
          # args: ["temporal-cassandra-tool", "update-schema", "-d", "/etc/temporal/schema/cassandra/visibility/versioned"]
          args: ['sh', '-c', 'temporal-cassandra-tool update-schema -d /etc/temporal/schema/cassandra/visibility/versioned']
          env:
            - name: CASSANDRA_HOST
              value: temporaltest-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal_visibility
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
